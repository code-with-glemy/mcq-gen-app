{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "01d3506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json\n",
    "import pandas as pd \n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "19197cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "770b6afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191cba80",
   "metadata": {},
   "source": [
    "#Creating an ChatOpenAI object with openai key , model and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59b712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY2), model_name=\"gpt-3.5-turbo\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e94de6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "49d83761",
   "metadata": {},
   "outputs": [],
   "source": [
    "Response_json = {\n",
    "    \"1\": {\n",
    "        \"question\": \"mutiple choice question\",\n",
    "        \"options\": {\"A\": \"choice here\", \"B\": \"choice here\", \"C\": \"choice here\", \"D\": \"choice here\"},\n",
    "        \"answer\": \"correct_answer\",\n",
    "        \"explanation\": \"explanation of the answer\",\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"question\": \"mutiple choice question\",\n",
    "        \"options\": {\"A\": \"choice here\", \"B\": \"choice here\", \"C\": \"choice here\", \"D\": \"choice here\"},\n",
    "        \"answer\": \"correct_answer\",\n",
    "        \"explanation\": \"explanation of the answer\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f0fe59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "Text:{text}\n",
    "You are an expert MCQ maker. Given the above text , it is your job to \\\n",
    "Create a quiz of {number} mulitple choice question for {subject} students in {tone} tone.\n",
    "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
    "Make sure to format your response like JSON below and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### JSON\n",
    "{response_json}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "811726b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=PromptTemplate(\n",
    "    input_variables=[\"text\",\"number\",\"subject\",\"tone\",\"response_json\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "98e5e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_chain=LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT,\n",
    "    output_key=\"quiz\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7ad9d654",
   "metadata": {},
   "outputs": [],
   "source": [
    "template2=\"\"\"\n",
    "You are an expert english grammarian and writer.Given a Multiple Choice Quiz for {subject} students.\\\n",
    "You need to evaluate the complexity of the question and give a complete analysis of the quiz.Only use at max 50 words for complexity\n",
    "if the quiz is not at par with the cognitive and analytical abilities of the students, \\\n",
    "update the quiz questions which needs to be changed and change the tone such that it prefectly fits the student ability.\n",
    "Quiz_MCQs:\n",
    "{quiz}\n",
    "\n",
    "Check from an expert English Writer for the above quiz:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5c760f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_evaluation_promt=PromptTemplate(\n",
    "    input_variables=[\"quiz\",\"subject\"], \n",
    "    template=template2,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b3629e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_review_chain=LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=quiz_evaluation_promt,\n",
    "    output_key=\"review\",\n",
    "    verbose=True\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "65202452",
   "metadata": {},
   "outputs": [],
   "source": [
    "SeqChain=SequentialChain(\n",
    "    chains=[quiz_chain,quiz_review_chain],\n",
    "    input_variables=[\"text\",\"number\",\"subject\",\"tone\",\"response_json\"],\n",
    "    output_variables=[\"quiz\",\"review\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c5f94d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_file_path = r\"C:\\\\Users\\\\gleme\\\\OneDrive\\\\Desktop\\\\GenAI-apps\\\\mcq-gen-app\\\\book.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "11993e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(book_file_path, \"rb\") as file:\n",
    "    Text=file.read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a3d89388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keezhadi, also spelt Keeladi , is a village near the village of Silaiman, u=\n",
      "nder the control of Manamadurai Municipalilty, situated on the border betwe=\n",
      "en Madurai and Sivagangai districts, in Tamil Nadu, India. The Keezhadi exc=\n",
      "avation site is located in this area. excavations carried out by the Archae=\n",
      "ological Survey of India (ASI) and the Tamil Nadu Archaeology Department (T=\n",
      "NAD) have revealed a Sangam era settlement dated to the 6th century BCE by =\n",
      "radiocarbon dating.[1] Claims that the results show that there was writing =\n",
      "at that time have been challenged. It is not clear whether the potsherds co=\n",
      "ntaining inscriptions were found in the same archaeological layer as the 6t=\n",
      "h century samples, and University of Calcutta archaeologist Bishnupriya Bas=\n",
      "ak said that \"This unfortunately is not clear from the report and is very c=\n",
      "rucial\", adding that the issues of \"layer, period and absolute dates\" neede=\n",
      "d clarity. Dravidian University archaeologist E. Harsha Vardhan said that a=\n",
      " single report was not enough to \"state scientifically that the Tamil-Brahm=\n",
      "i script belongs to the sixth century BC\".[2]\n",
      "\n",
      "Excavations\n",
      "Main article: Keezhadi excavation site\n",
      "An archaeological survey team under Archaeologist Amarnath Ramakrishna [3]=\n",
      " was first conducted in 2013 in the vicinity of the Vaigai river from Theni=\n",
      " district to Ramanathapuram district where the river meets the sea.[4] Duri=\n",
      "ng the study, 293 sites, including Keezhadi, were identified to have archae=\n",
      "ological residues.[5] The first three phases of excavation at Keezhadi were=\n",
      " conducted by the Archaeological Survey of India, and they dropped it from =\n",
      "doing further research. A public interest litigation was filed and followin=\n",
      "g that the court ordered the regional depart to carry forward, following wh=\n",
      "ich the fourth and fifth phases were conducted by the Tamil Nadu Archaeolog=\n",
      "y Department.\n",
      "\n",
      "Carbon nanomaterials in Keezhadi pottery\n",
      "A team of researchers identified pottery shards at Keezhadi that contain c=\n",
      "arbon nanomaterials, including single-walled and multi-walled carbon nanotu=\n",
      "bes.[6]\n",
      "\n",
      "Keezhadi Heritage Museum\n",
      "Keezhadi Heritage museum was inaugurated by Tamil Nadu Chief Minister M.K.=\n",
      " Stalin on 5 March 2023 in Sivaganga, close to the historic archaeological =\n",
      "site which was discovered in 2014. The museum has been established at a cos=\n",
      "t of =E2=82=B918.42 crore across 31,000 square feet of land. Built in a Kar=\n",
      "aikudi based traditional Chettinad style, the architecture displays artefac=\n",
      "ts and antiquities excavated from the site since 2017 in the present-day Si=\n",
      "vaganga district by the Tamil Nadu State Department of Archaeology.[7][8] O=\n",
      "n display would be artefacts like dice made of ivory and terracotta, male a=\n",
      "nd female figurines made of terracotta, iron dagger, and punch-mark coins. =\n",
      "The museum will also display replicas of the trenches and some of the urns =\n",
      "that were unearthed in Konthagai, believed to be the burial site of inhabit=\n",
      "ants of Keezhadi. The museum has six display halls =E2=80=93 only ground fl=\n",
      "oor in three, two with mezzanine floors and one with mezzanine and first fl=\n",
      "oor =E2=80=93 and an auditorium where visitors will be treated to documenta=\n",
      "ries on excavations in Keezhadi and their significance.[9]\n"
     ]
    }
   ],
   "source": [
    "print(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "94be1828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"1\": {\"question\": \"mutiple choice question\", \"options\": {\"A\": \"choice here\", \"B\": \"choice here\", \"C\": \"choice here\", \"D\": \"choice here\"}, \"answer\": \"correct_answer\", \"explanation\": \"explanation of the answer\"}, \"2\": {\"question\": \"mutiple choice question\", \"options\": {\"A\": \"choice here\", \"B\": \"choice here\", \"C\": \"choice here\", \"D\": \"choice here\"}, \"answer\": \"correct_answer\", \"explanation\": \"explanation of the answer\"}}'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(Response_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d0c60",
   "metadata": {},
   "source": [
    "Token usaga edocument - https://python.langchain.com/docs/how_to/chat_token_usage_tracking/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2d7af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d4070735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:Keezhadi, also spelt Keeladi , is a village near the village of Silaiman, u=\n",
      "nder the control of Manamadurai Municipalilty, situated on the border betwe=\n",
      "en Madurai and Sivagangai districts, in Tamil Nadu, India. The Keezhadi exc=\n",
      "avation site is located in this area. excavations carried out by the Archae=\n",
      "ological Survey of India (ASI) and the Tamil Nadu Archaeology Department (T=\n",
      "NAD) have revealed a Sangam era settlement dated to the 6th century BCE by =\n",
      "radiocarbon dating.[1] Claims that the results show that there was writing =\n",
      "at that time have been challenged. It is not clear whether the potsherds co=\n",
      "ntaining inscriptions were found in the same archaeological layer as the 6t=\n",
      "h century samples, and University of Calcutta archaeologist Bishnupriya Bas=\n",
      "ak said that \"This unfortunately is not clear from the report and is very c=\n",
      "rucial\", adding that the issues of \"layer, period and absolute dates\" neede=\n",
      "d clarity. Dravidian University archaeologist E. Harsha Vardhan said that a=\n",
      " single report was not enough to \"state scientifically that the Tamil-Brahm=\n",
      "i script belongs to the sixth century BC\".[2]\n",
      "\n",
      "Excavations\n",
      "Main article: Keezhadi excavation site\n",
      "An archaeological survey team under Archaeologist Amarnath Ramakrishna [3]=\n",
      " was first conducted in 2013 in the vicinity of the Vaigai river from Theni=\n",
      " district to Ramanathapuram district where the river meets the sea.[4] Duri=\n",
      "ng the study, 293 sites, including Keezhadi, were identified to have archae=\n",
      "ological residues.[5] The first three phases of excavation at Keezhadi were=\n",
      " conducted by the Archaeological Survey of India, and they dropped it from =\n",
      "doing further research. A public interest litigation was filed and followin=\n",
      "g that the court ordered the regional depart to carry forward, following wh=\n",
      "ich the fourth and fifth phases were conducted by the Tamil Nadu Archaeolog=\n",
      "y Department.\n",
      "\n",
      "Carbon nanomaterials in Keezhadi pottery\n",
      "A team of researchers identified pottery shards at Keezhadi that contain c=\n",
      "arbon nanomaterials, including single-walled and multi-walled carbon nanotu=\n",
      "bes.[6]\n",
      "\n",
      "Keezhadi Heritage Museum\n",
      "Keezhadi Heritage museum was inaugurated by Tamil Nadu Chief Minister M.K.=\n",
      " Stalin on 5 March 2023 in Sivaganga, close to the historic archaeological =\n",
      "site which was discovered in 2014. The museum has been established at a cos=\n",
      "t of =E2=82=B918.42 crore across 31,000 square feet of land. Built in a Kar=\n",
      "aikudi based traditional Chettinad style, the architecture displays artefac=\n",
      "ts and antiquities excavated from the site since 2017 in the present-day Si=\n",
      "vaganga district by the Tamil Nadu State Department of Archaeology.[7][8] O=\n",
      "n display would be artefacts like dice made of ivory and terracotta, male a=\n",
      "nd female figurines made of terracotta, iron dagger, and punch-mark coins. =\n",
      "The museum will also display replicas of the trenches and some of the urns =\n",
      "that were unearthed in Konthagai, believed to be the burial site of inhabit=\n",
      "ants of Keezhadi. The museum has six display halls =E2=80=93 only ground fl=\n",
      "oor in three, two with mezzanine floors and one with mezzanine and first fl=\n",
      "oor =E2=80=93 and an auditorium where visitors will be treated to documenta=\n",
      "ries on excavations in Keezhadi and their significance.[9]\n",
      "You are an expert MCQ maker. Given the above text , it is your job to Create a quiz of 2 mulitple choice question for History students in simple tone.\n",
      "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
      "Make sure to format your response like JSON below and use it as a guide. Ensure to make 2 MCQs\n",
      "### JSON\n",
      "{\"1\": {\"question\": \"mutiple choice question\", \"options\": {\"A\": \"choice here\", \"B\": \"choice here\", \"C\": \"choice here\", \"D\": \"choice here\"}, \"answer\": \"correct_answer\", \"explanation\": \"explanation of the answer\"}, \"2\": {\"question\": \"mutiple choice question\", \"options\": {\"A\": \"choice here\", \"B\": \"choice here\", \"C\": \"choice here\", \"D\": \"choice here\"}, \"answer\": \"correct_answer\", \"explanation\": \"explanation of the answer\"}}\n",
      "\u001b[0m\n",
      "Error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gleme\\AppData\\Local\\Temp\\ipykernel_60516\\2082888000.py\", line 3, in <module>\n",
      "    Response=SeqChain(\n",
      "             ^^^^^^^^^\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 189, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\langchain\\chains\\base.py\", line 386, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\langchain\\chains\\base.py\", line 167, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\langchain\\chains\\base.py\", line 157, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\langchain\\chains\\sequential.py\", line 107, in _call\n",
      "    outputs = chain(known_values, return_only_outputs=True, callbacks=callbacks)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 189, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\langchain\\chains\\base.py\", line 386, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\langchain\\chains\\base.py\", line 167, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\langchain\\chains\\base.py\", line 157, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 963, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 782, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1028, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py\", line 476, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py\", line 387, in completion_with_retry\n",
      "    return self.client.create(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1087, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\openai\\_base_client.py\", line 1249, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gleme\\OneDrive\\Desktop\\GenAI-apps\\mcq-gen-app\\env\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    try:\n",
    "        Response=SeqChain(\n",
    "        {   \"text\": Text,\n",
    "            \"number\": 2,\n",
    "            \"subject\": \"History\",\n",
    "            \"tone\": \"simple\",\n",
    "            \"response_json\": json.dumps(Response_json)\n",
    "        }\n",
    "        )\n",
    "        print(Response)\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
